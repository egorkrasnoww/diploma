import streamlit as st
import docx2txt
import pdfplumber
import tempfile
import pymorphy2
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch


morph = pymorphy2.MorphAnalyzer()


def normalize_text(text):
    text = re.sub(r"[^–∞-—è–ê-–Ø0-9\\s]", " ", str(text).lower())
    return " ".join([morph.parse(word)[0].normal_form for word in text.split()])


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ruT5
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("./finetuned_rut5_model", use_fast=False)
    model = AutoModelForSeq2SeqLM.from_pretrained("./finetuned_rut5_model")
    return tokenizer, model




tokenizer, model = load_model()


def summarize(text, level="—Å—Ä–µ–¥–Ω–µ–µ"):
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)

    if level == "–º—è–≥–∫–æ–µ":
        min_len, max_len, penalty = 100, 384, 0.8
    elif level == "–∂—ë—Å—Ç–∫–æ–µ":
        min_len, max_len, penalty = 20, 128, 2.2
    else:  # —Å—Ä–µ–¥–Ω–µ–µ
        min_len, max_len, penalty = 50, 256, 1.2

    summary_ids = model.generate(
        input_ids,
        max_length=max_len,
        min_length=min_len,
        length_penalty=penalty,
        num_beams=4,
        early_stopping=True
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)






def extract_text_from_docx(file):
    return docx2txt.process(file)


def extract_text_from_pdf(file):
    text = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

def split_text(text, max_tokens=480):
    sentences = text.split('. ')
    current, chunks = "", []

    for s in sentences:
        if len(tokenizer.encode(current + s)) < max_tokens:
            current += s + '. '
        else:
            chunks.append(current.strip())
            current = s + '. '
    if current:
        chunks.append(current.strip())
    return chunks



# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.set_page_config(page_title="NLP –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", page_icon="üìÑ", layout="wide")

st.markdown(
    "<h1 style='text-align: center; color: #2C3E50;'>üìÑ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞</h1>",
    unsafe_allow_html=True,
)

st.markdown(
    "<p style='text-align: center; font-size: 16px; color: gray;'>–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç, –∏ –º–æ–¥–µ–ª—å —Å–æ–∫—Ä–∞—Ç–∏—Ç –µ–≥–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–º—ã—Å–ª–∞</p>",
    unsafe_allow_html=True,
)


col1, col2 = st.columns([1, 2])

with col1:
    uploaded_file = st.file_uploader("üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (.txt, .docx, .pdf)", type=["txt", "docx", "pdf"])

with col2:
    st.info("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –æ—Ç—á—ë—Ç—ã, –∞–∫—Ç—ã, —Å–ª—É–∂–µ–±–Ω—ã–µ –∑–∞–ø–∏—Å–∫–∏. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ ‚Äî 512 —Ç–æ–∫–µ–Ω–æ–≤.")


st.markdown("### ‚úÇÔ∏è –£—Ä–æ–≤–µ–Ω—å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞")
reduction_level = st.selectbox(
    "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–µ–ø–µ–Ω—å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è:",
    ["–º—è–≥–∫–æ–µ", "—Å—Ä–µ–¥–Ω–µ–µ", "–∂—ë—Å—Ç–∫–æ–µ"]
)

if uploaded_file:
    file_type = uploaded_file.name.split('.')[-1].lower()

    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp:
        tmp.write(uploaded_file.read())
        file_path = tmp.name

    if file_type == "txt":
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
    elif file_type == "docx":
        text = extract_text_from_docx(file_path)
    elif file_type == "pdf":
        text = extract_text_from_pdf(file_path)
    else:
        st.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.")
        text = ""

    if text:
        st.subheader("üìë –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
        st.text_area("–¢–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:", text, height=300)

        if st.button("üöÄ –°–æ–∫—Ä–∞—Ç–∏—Ç—å"):
            with st.spinner("–ú–æ–¥–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç..."):
                result = summarize(text, level=reduction_level)


                original_word_count = len(text.split())
                summary_word_count = len(result.split())
                reduction_percent = round(100 * (original_word_count - summary_word_count) / original_word_count)


                st.subheader("‚úÖ –°–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
                st.success(result)

                st.markdown(f"""
                ### üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è:
                - –ë—ã–ª–æ —Å–ª–æ–≤: **{original_word_count}**
                - –°—Ç–∞–ª–æ —Å–ª–æ–≤: **{summary_word_count}**
                - üîª –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ: **{reduction_percent}%**
                """)

else:
    st.warning("‚¨Ü –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É.")
