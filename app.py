# --- –ò–º–ø–æ—Ä—Ç—ã ---
import streamlit as st
import docx2txt
import pdfplumber
import tempfile
import pymorphy2
import re
from docx import Document
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ---
morph = pymorphy2.MorphAnalyzer()
def normalize_text(text):
    text = re.sub(r"[^–∞-—è–ê-–Ø0-9\\s]", " ", str(text).lower())
    return " ".join([morph.parse(word)[0].normal_form for word in text.split()])

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("./finetuned_rut5_model", use_fast=False)
    model = AutoModelForSeq2SeqLM.from_pretrained("./finetuned_rut5_model")
    return tokenizer, model

tokenizer, model = load_model()

# --- –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ---
def summarize(text, level="—Å—Ä–µ–¥–Ω–µ–µ"):
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)
    if level == "–º—è–≥–∫–æ–µ":
        min_len, max_len, penalty = 100, 384, 0.8
    elif level == "–∂—ë—Å—Ç–∫–æ–µ":
        min_len, max_len, penalty = 20, 128, 2.2
    else:
        min_len, max_len, penalty = 50, 256, 1.2
    summary_ids = model.generate(
        input_ids,
        max_length=max_len,
        min_length=min_len,
        length_penalty=penalty,
        num_beams=4,
        early_stopping=True
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ DOCX ---
def save_to_docx(text):
    doc = Document()
    doc.add_paragraph(text)
    path = "summary.docx"
    doc.save(path)
    return path

# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤ ---
def extract_text_from_docx(file): return docx2txt.process(file)
def extract_text_from_pdf(file):
    text = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# --- –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –±–ª–æ–∫–∏ ---
def split_text(text, max_tokens=480):
    sentences = text.split('. ')
    current, chunks = "", []
    for s in sentences:
        if len(tokenizer.encode(current + s)) < max_tokens:
            current += s + '. '
        else:
            chunks.append(current.strip())
            current = s + '. '
    if current:
        chunks.append(current.strip())
    return chunks

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
st.set_page_config(page_title="NLP –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", page_icon="üìÑ", layout="wide")

# --- –¶–≤–µ—Ç–æ–≤–∞—è —Ç–µ–º–∞ ---
st.markdown("""
<style>
body, .stApp {
    background-color: #F9FAFC;
    color: #1B1F3B;
}
h1, h2, h3, h4, h5, h6 {
    color: #0A4D8C;
}
.stButton>button {
    background-color: #0A4D8C;
    color: white;
    border-radius: 8px;
    padding: 0.5em 1em;
    border: none;
    font-weight: bold;
}
.stSelectbox>div>div {
    background-color: #EAF2FB;
    color: #0A4D8C;
}
.stTextArea textarea {
    background-color: #FFFFFF;
    color: #1B1F3B;
}
.stInfo, .stWarning {
    background-color: #EAF2FB;
    color: #0A4D8C;
}
</style>
""", unsafe_allow_html=True)

# --- –ó–∞–≥–æ–ª–æ–≤–æ–∫ ---
st.markdown("""
<h1 style='text-align: center; color: #0A4D8C;'>üìÑ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞</h1>
<p style='text-align: center; font-size: 16px; color: #1B1F3B;'>–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ –≤—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é ‚Äî –º–æ–¥–µ–ª—å —Å–æ–∫—Ä–∞—Ç–∏—Ç –µ–≥–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Å–º—ã—Å–ª–∞.</p>
""", unsafe_allow_html=True)

# --- –†–µ–∂–∏–º –≤–≤–æ–¥–∞ ---
st.markdown("### üîΩ –í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞")
input_mode = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ —Ç–µ–∫—Å—Ç–∞:", ["–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª", "–í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é"])

text = ""

# --- –í–≤–æ–¥ –≤—Ä—É—á–Ω—É—é ---
if input_mode == "–í–≤–µ—Å—Ç–∏ –≤—Ä—É—á–Ω—É—é":
    text = st.text_area("‚úçÔ∏è –í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è", height=300)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ---
else:
    uploaded_file = st.file_uploader("üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (.txt, .docx, .pdf)", type=["txt", "docx", "pdf"])
    if uploaded_file:
        file_type = uploaded_file.name.split('.')[-1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp:
            tmp.write(uploaded_file.read())
            file_path = tmp.name
        if file_type == "txt":
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
        elif file_type == "docx":
            text = extract_text_from_docx(file_path)
        elif file_type == "pdf":
            text = extract_text_from_pdf(file_path)
        else:
            st.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.")
            text = ""

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ---
if text:
    st.markdown("### ‚úÇÔ∏è –£—Ä–æ–≤–µ–Ω—å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞")
    reduction_level = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–µ–ø–µ–Ω—å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è:", ["–º—è–≥–∫–æ–µ", "—Å—Ä–µ–¥–Ω–µ–µ", "–∂—ë—Å—Ç–∫–æ–µ"])

    st.subheader("üìë –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
    st.text_area("–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:", text, height=300)

    if st.button("üöÄ –°–æ–∫—Ä–∞—Ç–∏—Ç—å"):
        with st.spinner("–ú–æ–¥–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç..."):
            chunks = split_text(text)
            result_chunks = []
            progress_bar = st.progress(0)
            for i, chunk in enumerate(chunks):
                summary = summarize(chunk, level=reduction_level)
                result_chunks.append(summary)
                progress_bar.progress((i + 1) / len(chunks))
            result = "\n\n".join(result_chunks)

        original_word_count = len(text.split())
        summary_word_count = len(result.split())
        reduction_percent = round(100 * (original_word_count - summary_word_count) / original_word_count)

        st.subheader("‚úÖ –°–æ–∫—Ä–∞—â—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
        st.success(result)

        st.markdown(f"""
        ### üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è:
        - –ë—ã–ª–æ —Å–ª–æ–≤: **{original_word_count}**
        - –°—Ç–∞–ª–æ —Å–ª–æ–≤: **{summary_word_count}**
        - üîª –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ: **{reduction_percent}%**
        """)

        # --- –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
        col1, col2 = st.columns(2)
        with col1:
            st.download_button("üíæ –°–∫–∞—á–∞—Ç—å .txt", result, file_name="summary.txt", mime="text/plain")
        with col2:
            docx_path = save_to_docx(result)
            with open(docx_path, "rb") as file:
                st.download_button("üìÑ –°–∫–∞—á–∞—Ç—å .docx", file.read(), file_name="summary.docx", mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

else:
    st.warning("‚¨Ü –í—Å—Ç–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
